\begin{itemize}
    \item In all of our experiments, MilNet outperformed all the baselines. This shows that it is indeed a valid technique for the sentiment analysis that can lead to good results.
    \item Even when they come from the same domain and context, {\bf Amazon EN} and {\bf Amazon DE} have notorious differences because of the language. This is reflected not only in the structure of the sentences themselves, but also in the results obtained in our classification experiments. For some experiments, results for the German data are better, possibly, due to the features of the language and the quality of the dataset.
    \item In our setting, comment-level context for embeddings didn't produce significantly better results than sentence-level context.
    \item For most of the experiments, different embedding produced similar results. In the overall result, XLING produced the best results for the task. Despite this, this result can not be generalized, because the embedding selection depends heavily on the task and data used.
    \item Presence of {\it 'neutral'} sentiment makes the task much harder for the models and baselines. This is probably because models choose to predict {\it 'neutral'} when unsure about the decision.
    \item Originally, the idea of the project was to train a model on {\bf amazon EN} and then fine-tune in on {\bf organic}. This approach gave just a small improvement on the results of pure {\bf organic} training;
    \item Neither Amazon nor organic data use the whole power of the MilNet architecture. We have only comment-level labels for the Amazon data, hence, we cannot properly test the performance on the task of predicting sentiments for individual sentences. For the organic data, the situation is opposite: we have only sentence-level labels, so we cannot properly train all the levels of the model.
\end{itemize}