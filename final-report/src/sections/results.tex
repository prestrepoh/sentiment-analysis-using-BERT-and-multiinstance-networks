We were using both micro and macro F1 scores as target metrics.
\subsection{Monolingual}
We trained our network on BERT base embeddings for two tasks: \taskEN and \taskORG. The main interest was to compare fine-tuned and not fine-tuned models. \\ 
\input{plots/monolingual_finetune}
As you can see in Figure \ref{monolingual_finetune}, training only on \dataEN yields poor results for \dataORG, while fine-tuning on \dataORG significantly drops the performance on \dataEN. Such effect may occur because comments in \dataEN and \dataORG have different structure or they've been annotated in slightly different ways. \\
Figure \ref{monolingual_finetune} shows that we should not fine-tune the model if we want to get the best results for \dataEN. Thus, from now on task \taskORG will always imply that fine-tuning on \dataORG was performed; task \taskEN -- that it was not. \\
\input{plots/monolingual_results}
In Figure \ref{monolingual_results} you can see the results for all the monolingual experiments. The plots show that task \taskORGORG is easier than \taskORG not only for our model but for the baselines as well. Again, it may be the evidence of some notable distinctions between \dataEN and \dataORG.
\subsection{Cross-lingual}
Figure \ref{crosslingual_results} shows the results for our cross-lingual experiments performed on different initial embeddings. We can see that for the first two tasks all the embeddings produced similar results. Interestingly, although \dataEN was used only as a test set, all the embeddings yielded better F1-micro scores for \taskDE than for \taskEN. Possibly, this result could be explained by the quality of \taskDE and the features of the German language --- for example, one could suggest that Germans express their opinions in a clearer way.
\input{plots/crosslingual_results}
\subsection{Different context level embeddings}
In our next experiment, we compared two models trained on BERT multilingual embeddings with different levels of context: "comment as a context"\ and "sentence as a context". As was described in section \ref{contexLeveLEmbeddings}, for getting valid embeddings we had to discard a large portion of the data. Thus, the results in the Figure \ref{crosslingual_context} for \dataEN are worse than the ones shows in the Figure \ref{crosslingual_results}. \\
Note that comment-level context coincides with sentence-level context for comments having only one sentence. For this reason, we didn't run this experiment for \dataORG. \\
As you can see in Figure \ref{crosslingual_context}, for our task comment-level context didn't cause any significant improvement in the results.
\input{plots/crosslingual_context}
\subsection{Two-class}
As our last step, we trained a model for predicting one of the two sentiments: {\it negative} or {\tt positive}. 
\input{plots/two_class_results}
Figure \ref{two_class_results} shows that for all the experiments reducing the number of classes triggers major improvements in F1-macro score. Similarly to the Figure \ref{crosslingual_results}, for the two-class problem \taskDE produces better results than \taskEN.